{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:07.902061Z",
     "start_time": "2025-09-10T21:02:07.898899Z"
    }
   },
   "source": [
    "import boto3, re, json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:09.188738Z",
     "start_time": "2025-09-10T21:02:09.186188Z"
    }
   },
   "source": [
    "result_path_pattern = re.compile(r\"result-partition-(?P<partition>[0-9]+MB)/(?P<data_size>[0-9]+GB)/(run )?(?P<run_no>[0-9]+)\")\n",
    "search_result = result_path_pattern.search(\"result-partition-75MB/1GB/1\")\n",
    "search_dict = search_result.groupdict()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:10.777605Z",
     "start_time": "2025-09-10T21:02:10.775413Z"
    }
   },
   "source": [
    "pattern = re.compile(r\"result-partition-[0-9]+MB/((total)|([0-9]+GB))/(run )?[0-9]+\")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:12.581448Z",
     "start_time": "2025-09-10T21:02:12.568658Z"
    }
   },
   "source": "def get_event_details(event):\n    return event.get('executionSucceededEventDetails') or \\\n        event.get('executionFailedEventDetails') or \\\n        event.get('executionStartedEventDetails') or \\\n        event.get('stateEnteredEventDetails') or {}\n\ndef extract_event_details(history, results, verbose):\n    state = 0\n    execution_start, execution_end = None, None\n    inference_start, inference_end = None, None\n    result_path = None\n\n    # Print execution events\n    for event in history['events']:\n        timestamp = event['timestamp']\n        event_type = event['type']\n        details = get_event_details(event)\n        \n        if verbose:print(f\"{timestamp} - {event_type}\")\n        if details is not None and verbose:\n            print(f\"  Details: {details}\")\n            \n        try:\n            if event_type == 'ExecutionStarted': \n                execution_start = timestamp\n            elif event_type == 'ExecutionSucceeded': execution_end = timestamp\n            elif event_type == 'TaskStateEntered': \n                if state ==0:\n                    input_json = json.loads(details['input'])\n                    result_path = input_json['result_path']\n                    # print(f\"{timestamp}: Result Path: {result_path}\")\n                \n                    # if pattern.fullmatch(result_path) is None:\n                    if \"demo\" in result_path or result_path in results['result_path']:\n                        # print(f\"Skipping {result_path}.\")\n                        return results\n                    \n                    result_path_splitted = result_path.split('/')\n                    # \"result-partition-100MB/1GB/1\" or \"result-partition-100MB/1GB/run 1\"\n                    \n                    if 'Batches' in result_path:\n                        data_size = result_path_splitted[-4]\n                    else:\n                        data_size = result_path_splitted[-2]\n                    if data_size == 'total': data_size = '12.6GB'\n                    \n                    run_no = result_path_splitted[-1].replace('run ', '')\n                    partition_size = input_json['data_prefix']\n                    batch_size = input_json['batch_size']\n                \n                state += 1\n            elif event_type == 'MapStateEntered':\n                # input_json = json.loads(details['input'])\n                 \n                inference_start = timestamp\n                state += 1\n            elif event_type == 'MapStateExited': \n                inference_end = timestamp\n                state += 1\n                \n        except Exception as e:\n            print(f\"  Error {e.with_traceback}\")\n            return results\n            \n    if verbose: print(\"-\" * 80)\n    \n    delta = (execution_end - execution_start)\n    total_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n    if verbose: print(f\"Total Duration: {total_duration:.2f} seconds.\")\n\n    delta = (inference_end - inference_start)\n    inference_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n    if verbose: print(f\"Inference Duration: {inference_duration:.2f} seconds.\")\n\n    results['result_path'].append(result_path)\n    results['data (GB)'].append(data_size)\n    results['run'].append(run_no)\n    results['partition (MB)'].append(partition_size)\n    results['total_duration (s)'].append(total_duration)\n    results['inference_duration (s)'].append(inference_duration)\n    results['batch_size'].append(batch_size)\n    results['batch_varying'].append('Batches' in result_path)\n    \n    return results\n\ndef get_step_function_logs(\n    state_machine_arn, start_date=None, end_date=None,\n    verbose=False, profile_name=None\n):\n    \"\"\"\n    Collects the events log for a specific AWS Step Functions state machine.\n\n    :param state_machine_arn: The ARN of the Step Functions state machine\n    :param start_date: Optional, filter logs starting from this date (timezone-aware datetime object)\n    :param end_date: Optional, filter logs until this date (timezone-aware datetime object)\n    :param profile_name: Optional, AWS profile name to use\n    \"\"\"\n    # Initialize boto3 session with profile if provided\n    if profile_name:\n        session = boto3.Session(profile_name=profile_name)\n        stepfunctions_client = session.client('stepfunctions', region_name='us-east-1')\n    else:\n        stepfunctions_client = boto3.client('stepfunctions', region_name='us-east-1')\n        \n    results = {\n        key: [] for key in [\n            'result_path', 'data (GB)','batch_size', 'run', \n            'partition (MB)', 'total_duration (s)', \n            'inference_duration (s)', 'batch_varying']\n    }\n\n    try:\n        # List executions for the state machine\n        executions = stepfunctions_client.list_executions(\n            stateMachineArn=state_machine_arn,\n            statusFilter='SUCCEEDED',  # You can filter by RUNNING, FAILED, etc.\n            maxResults=1000 # update if you have more than 1000 executions\n        )\n\n        print(f\"Fetching logs for Step Functions state machine: {state_machine_arn}\\n\")\n        print(f\"Found {len(executions['executions'])} executions.\\n\")\n\n        # Iterate through executions\n        for execution in executions['executions']:\n            execution_arn = execution['executionArn']\n            start_time = execution['startDate']\n            stop_time = execution['stopDate']\n            \n            # Ensure start_date and end_date are timezone-aware and in UTC\n            if start_date:\n                start_date = start_date.astimezone(pytz.utc)\n            if end_date:\n                end_date = end_date.astimezone(pytz.utc)\n\n            # Filter by start_date and end_date if provided\n            if start_date and start_time < start_date:\n                continue\n            if end_date and stop_time > end_date:\n                continue\n            \n            if verbose: print(f\"Execution ARN: {execution_arn}\")\n            if verbose: print(f\"Start Time: {start_time}, Stop Time: {stop_time}\")\n\n            # Get execution history\n            history = stepfunctions_client.get_execution_history(\n                executionArn=execution_arn,\n                reverseOrder=False  # Set to True if you want events in reverse order\n            )\n            \n            results = extract_event_details(history, results, verbose)\n            # break\n            \n    except stepfunctions_client.exceptions.ResourceNotFound:\n        print(f\"The state machine ARN {state_machine_arn} does not exist.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        \n    del results['result_path']\n    return results",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:27.848065Z",
     "start_time": "2025-09-10T21:02:23.253063Z"
    }
   },
   "source": [
    "state_machine_arn = \"arn:aws:states:us-east-1:448324707516:stateMachine:DataParallel-CosmicAI\"\n",
    "\n",
    "# Optional: Define a time range (use timezone-aware UTC datetimes)\n",
    "start_date = datetime(2024, 11, 11, 0, 0, 0, tzinfo=pytz.utc)  # Example: Start from this date\n",
    "end_date = None # datetime(2024, 11, 22, 23, 0, 0, tzinfo=pytz.utc)    # Example: Until this date\n",
    "\n",
    "results = get_step_function_logs(state_machine_arn, start_date, end_date, profile_name= \"cylon\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching logs for Step Functions state machine: arn:aws:states:us-east-1:448324707516:stateMachine:DataParallel-CosmicAI\n",
      "\n",
      "Found 29 executions.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:29.429844Z",
     "start_time": "2025-09-10T21:02:29.419747Z"
    }
   },
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df[(df['data (GB)'] != '100MB') & (df['partition (MB)'] != 'data')]\n",
    "# because the first ones are latest\n",
    "df.drop_duplicates(subset=['data (GB)', 'batch_size', 'run', 'partition (MB)', 'batch_varying'], inplace=True, keep='first')\n",
    "df.head(30)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data (GB)  batch_size run partition (MB)  total_duration (s)  \\\n",
       "0      983GB         512   3          100MB             109.010   \n",
       "1      983GB         512   2          100MB             109.204   \n",
       "2      983GB         512   1          100MB             116.807   \n",
       "3        1TB         512   3          100MB             220.969   \n",
       "4        1TB         512   2          100MB             242.601   \n",
       "5        1TB         512   1          100MB             246.868   \n",
       "6      100GB         512   3          100MB              73.525   \n",
       "8      100GB         512   2          100MB              58.088   \n",
       "9      100GB         512   1          100MB              59.686   \n",
       "11       2GB         512   1           50MB              58.628   \n",
       "\n",
       "    inference_duration (s)  batch_varying  \n",
       "0                   58.997          False  \n",
       "1                   58.983          False  \n",
       "2                   65.134          False  \n",
       "3                  163.889          False  \n",
       "4                  184.574          False  \n",
       "5                  193.245          False  \n",
       "6                   63.339          False  \n",
       "8                   51.262          False  \n",
       "9                   52.764          False  \n",
       "11                  50.799          False  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data (GB)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>run</th>\n",
       "      <th>partition (MB)</th>\n",
       "      <th>total_duration (s)</th>\n",
       "      <th>inference_duration (s)</th>\n",
       "      <th>batch_varying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100MB</td>\n",
       "      <td>109.010</td>\n",
       "      <td>58.997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100MB</td>\n",
       "      <td>109.204</td>\n",
       "      <td>58.983</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>983GB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100MB</td>\n",
       "      <td>116.807</td>\n",
       "      <td>65.134</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1TB</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100MB</td>\n",
       "      <td>220.969</td>\n",
       "      <td>163.889</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1TB</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100MB</td>\n",
       "      <td>242.601</td>\n",
       "      <td>184.574</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1TB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100MB</td>\n",
       "      <td>246.868</td>\n",
       "      <td>193.245</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100GB</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100MB</td>\n",
       "      <td>73.525</td>\n",
       "      <td>63.339</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100GB</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100MB</td>\n",
       "      <td>58.088</td>\n",
       "      <td>51.262</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100GB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100MB</td>\n",
       "      <td>59.686</td>\n",
       "      <td>52.764</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2GB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>50MB</td>\n",
       "      <td>58.628</td>\n",
       "      <td>50.799</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:39.728170Z",
     "start_time": "2025-09-10T21:02:39.723643Z"
    }
   },
   "source": "# Convert TB to GB first, then remove units and convert to float\ndf['data (GB)'] = df['data (GB)'].str.replace('TB', '000').str.replace('GB', '').astype(float)\ndf['partition (MB)'] = df['partition (MB)'].str.replace('MB', '').astype(int)\ndf['num_worlds'] = ((df['data (GB)'] * 1024 + df['partition (MB)'] -1 ) // df['partition (MB)']).astype(int)",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:43.181135Z",
     "start_time": "2025-09-10T21:02:43.174373Z"
    }
   },
   "source": [
    "df = df[['partition (MB)', 'data (GB)', 'batch_size', 'batch_varying', 'run', 'num_worlds', 'total_duration (s)', 'inference_duration (s)']]\n",
    "df.sort_values(by=['partition (MB)', 'data (GB)','batch_size','batch_varying', 'run'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:02:47.578804Z",
     "start_time": "2025-09-10T21:02:47.567971Z"
    }
   },
   "source": [
    "df.round(2).to_csv('./results/state_machine_logs.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
